{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LnFjWSx1jMp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import h2o\n",
    "import seaborn as sns\n",
    "sys.path.insert(0, '..')\n",
    "from DataModule.Data_Preparation import CoronnaCERTAINDataset\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def responseClassify_binary(row, baseline, _next):\n",
    "    # set threshold\n",
    "    lower_change = 0.6\n",
    "    upper_change = 1.2\n",
    "\n",
    "    change = row[_next]\n",
    "    row[_next] = row[baseline] - change\n",
    "\n",
    "    if change <= lower_change:\n",
    "        return \"Nonresponder\"\n",
    "\n",
    "    elif (change <= upper_change) & (change > lower_change):\n",
    "        if row[_next] > 5.1:\n",
    "            return \"Nonresponder\"\n",
    "        else:\n",
    "            return \"Responder\"\n",
    "\n",
    "    elif change > upper_change:\n",
    "        if row[_next] > 3.2:\n",
    "            return \"Responder\"\n",
    "        else:\n",
    "            return \"Responder\"\n",
    "\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def MSE(true, pred):\n",
    "    return metrics.mean_squared_error(true, pred)\n",
    "    \n",
    "def Classification_Accuracy(true, pred):\n",
    "    return metrics.accuracy_score(true, pred)\n",
    "\n",
    "def R2(true, pred):\n",
    "    return metrics.r2_score(true, pred)\n",
    "\n",
    "def Adjusted_R2(true, pred, p):\n",
    "    n = len(true)\n",
    "    return 1-((1-R2(true,pred))*(n-1))/(n-p-1)\n",
    "\n",
    "def F1_Score(true, pred):\n",
    "    return metrics.f1_score(true, pred, average='macro')\n",
    "\n",
    "def RPT(stability,performance,beta=1):\n",
    "    return (beta**2+1)*stability*performance / (beta**2*stability+performance)\n",
    "\n",
    "def calculate_RPT(row, metrics):\n",
    "    stability = row[f'{metrics}_std']\n",
    "    performance = row[f'{metrics}_mean']\n",
    "    if metrics == 'mse':\n",
    "        performance = 1 / performance\n",
    "        stability = 1 / stability\n",
    "    return RPT(stability,performance,beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../leaderboard/output.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model_family(row):\n",
    "    if \"StackedEnsemble\" in row['model_id']:\n",
    "        return '_'.join(row['model_id'].split('_',3)[:2])\n",
    "    else:\n",
    "        return row['model_id'].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev = df.drop(columns=['process_approach','imputation','patient_group','drug_group','train_test_rate','remove_low_DAS','random_state'])\n",
    "df_dev = df\n",
    "df_dev.loc[:,'model_family'] = df_dev.apply(lambda row:add_model_family(row),axis=1)\n",
    "# df_dev.loc[:,'RPT'] = df_dev.apply(lambda row: calculate_RPT(row,'mse'), axis=1)\n",
    "# df_dev = df_dev.sort_values(by='mse_mean',ascending=True).reset_index(drop=True)\n",
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.to_csv(\"../imputation_comparison.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.sort_values(by='mse_mean',ascending=True)[['model_id','mse_mean','mse_std']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15FjtJ6bHeLE"
   },
   "outputs": [],
   "source": [
    "def error_bar_plot(df, evaluation_metrics):\n",
    "    if evaluation_metrics == 'r2' or evaluation_metrics == 'RPT':\n",
    "        selected_models = df.sort_values(by=f'{evaluation_metrics}_mean',ascending=False).groupby('model_family').first()\n",
    "        selected_models = selected_models.sort_values(by=f'{evaluation_metrics}_mean', ascending=False)\n",
    "        y = selected_models[f'{evaluation_metrics}_mean']\n",
    "        x = y.index\n",
    "        y_error = selected_models[f'{evaluation_metrics}_std']\n",
    "    else:\n",
    "        selected_models = df.sort_values(by=f'{evaluation_metrics}_mean',ascending=True).groupby('model_family').first()\n",
    "        selected_models = selected_models.sort_values(by=f'{evaluation_metrics}_mean', ascending=True)\n",
    "        y = selected_models[f'{evaluation_metrics}_mean']\n",
    "        x = y.index\n",
    "        y_error = selected_models[f'{evaluation_metrics}_std']\n",
    "   \n",
    "    y_1 = [round(a,2) for a in y]\n",
    "    y_perc = [round(a*100,1) for a in y]\n",
    "#     print(min(y_error[:2]))\n",
    "    colors = [\"#F9665E\" if i == 0 else \"#799FCB\" for i in range(len(x))]\n",
    "    \n",
    "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "    fig.set_size_inches(20,6)\n",
    "\n",
    "    if evaluation_metrics == 'mse':\n",
    "        ax1.set_ylim(0,max(y+y_error)+0.1)\n",
    "        bars = ax1.bar(x,y_1,color=colors)\n",
    "    elif evaluation_metrics == 'accuracy':\n",
    "        ax1.set_ylim(50,100)\n",
    "        bars = ax1.bar(x,y_perc,color=colors)\n",
    "    elif evaluation_metrics == 'RPT':\n",
    "        ax1.set_ylim(1,1.75)\n",
    "        bars = ax1.bar(x,y_1,color=colors)\n",
    "    else:\n",
    "        bars = ax1.bar(x,y,color=colors)\n",
    "        \n",
    "    ax1.set_xlabel('Model', fontsize=20)\n",
    "    ax1.set_ylabel(evaluation_metrics.upper(), fontsize=20)\n",
    "    \n",
    "    if evaluation_metrics == 'RPT':\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        ax1.bar_label(bars, label_type='center',color='white', fontsize=12)\n",
    "\n",
    "        ax1.errorbar(x, y, yerr=y_error,\n",
    "                  fmt='o', color='orange', ecolor='orange',\n",
    "                  elinewidth = 3, capsize=10)\n",
    "        extent = ax2.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "        fig.savefig('mse.png', bbox_inches=extent)\n",
    "        \n",
    "        ax2.scatter(y,y_error)\n",
    "        for i in range(len(x)):\n",
    "            alignment = 'left' if y[i] < 1.22 else 'right'\n",
    "            ax2.text(x=y[i], y=y_error[i], s=x[i], \n",
    "                     horizontalalignment=alignment, verticalalignment='bottom',\n",
    "                     fontdict=dict(color='black', alpha=0.8, size=16))\n",
    "        ax2.plot([min(y), max(y)], [min(y_error), max(y_error)], ls=\"--\", c=\".3\")\n",
    "        ax2.set_xlabel(f'{evaluation_metrics}_mean', fontsize=20)\n",
    "        ax2.set_ylabel(f'{evaluation_metrics}_std', fontsize=20)\n",
    "    \n",
    "    plt.setp(ax1.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "    \n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.2, \n",
    "                    hspace=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k, top_k_error, test = filter_data(rank_metrix='MSE',challenge='regression',topk=5)\n",
    "error_bar_plot(df_dev, 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CoronnaCERTAINDataset(\n",
    "    library_root = 'Dataset/',\n",
    "    challenge = 'regression_delta', #option: regression, regression_delta, classification, binary_classification, regression_delta_binary\n",
    "    dataset = 'CORRONA CERTAIN', \n",
    "    process_approach = 'SC', #option: KVB, SC\n",
    "    imputation = None, #option: SimpleFill, KNN, SoftImpute, BiScaler, NuclearNormMinimization, IterativeImputer, IterativeSVD, None(raw)\n",
    "    patient_group = ['bionaive TNF'], #option: \"all\", \"bioexp nTNF\", \"bionaive TNF\", \"bionaive orencia\", \"KVB\"\n",
    "    drug_group = 'all', #option: \"all\", \"actemra\", \"cimzia\", \"enbrel\", \"humira\", \"orencia\", \"remicade\", \"rituxan\", \"simponi\"\n",
    "    time_points = (0,3), \n",
    "    train_test_rate = 0.8,\n",
    "    remove_low_DAS = True,\n",
    "    save_csv = False, \n",
    "    random_state = 2022,\n",
    "    verbose=False)\n",
    "\n",
    "train, train_loc = dataset.get_train()\n",
    "test, test_loc = dataset.get_test()\n",
    "\n",
    "# Start the H2O cluster (locally)\n",
    "h2o.init()\n",
    "\n",
    "# Import a sample binary outcome train/test set into H2O\n",
    "# train_h2o = h2o.upload_file(str(train_loc))\n",
    "# test_h2o = h2o.upload_file(str(test_loc))\n",
    "train_h2o = h2o.import_file(str(train_loc))\n",
    "test_h2o = h2o.import_file(str(test_loc))\n",
    "\n",
    "# Identify predictors and response\n",
    "x = train_h2o.columns[:-1]\n",
    "# y = \"DAS28_CRP_3M\"\n",
    "y = dataset.target\n",
    "\n",
    "for feature in dataset.categorical:\n",
    "    train_h2o[feature] = train_h2o[feature].asfactor()\n",
    "    test_h2o[feature] = test_h2o[feature].asfactor()\n",
    "if \"classification\" in dataset.challenge:\n",
    "    train_h2o[y] = train_h2o[y].asfactor()\n",
    "    test_h2o[y] = test_h2o[y].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "evaluation_metrics = 'mse'\n",
    "model_path_dir = '../leaderboard/model_saved/'\n",
    "model_id_list = list(df_dev.loc[df_dev.groupby('model_family')[f'{evaluation_metrics}_mean'].idxmin()].reset_index(drop=True).sort_values(by=f'{evaluation_metrics}_mean')['model_id'].values)\n",
    "# model_id = model_id_list[0]\n",
    "for i, model_id in enumerate(model_id_list):\n",
    "    model_path = os.path.join(model_path_dir,model_id)\n",
    "    uploaded_model = h2o.upload_model(model_path)\n",
    "    m = h2o.get_model(model_id)\n",
    "    print(m)\n",
    "    print(m.model_performance(test_h2o))\n",
    "    regression_pred = m.predict(test_h2o).as_data_frame()['predict']\n",
    "    regression_true = test_h2o.as_data_frame()[dataset.target]\n",
    "    m.model_performance(test_h2o)\n",
    "\n",
    "    X = test.drop(columns=dataset.target)\n",
    "    true = test[dataset.target]\n",
    "    pred = m.predict(test_h2o).as_data_frame()\n",
    "    \n",
    "    print(\"R2\", R2(true,pred))\n",
    "    print(\"Adjusted_R2\", Adjusted_R2(true,pred,len(test.columns)))\n",
    "    print()\n",
    "\n",
    "    baseline = test['DAS28_CRP_0M']\n",
    "\n",
    "    baseline, true, pred = np.array(baseline), np.array(true), np.squeeze(np.array(pred))\n",
    "    results_df = pd.DataFrame(list(zip(baseline, true, pred)),\n",
    "                      columns=['baseline', 'true', 'pred'])\n",
    "\n",
    "    classification_pred = results_df.apply(\n",
    "        lambda row: responseClassify_binary(row, 'baseline', 'pred'), axis=1)\n",
    "\n",
    "    classification_true = results_df.apply(\n",
    "        lambda row: responseClassify_binary(row, 'baseline', 'true'), axis=1)\n",
    "\n",
    "    print(\"classification accuracy\", Classification_Accuracy(classification_true,classification_pred))\n",
    "    print(\"F1 score\", F1_Score(classification_true,classification_pred))\n",
    "\n",
    "    contingency_matrix = pd.crosstab(classification_true, classification_pred, rownames=['true'], colnames=[\n",
    "                                                 'prediction'], normalize='columns')\n",
    "    plt.figure(i)\n",
    "    sns.heatmap(contingency_matrix.T, annot=True,\n",
    "                fmt='.2f', cmap=\"YlGnBu\", cbar=False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../leaderboard/model_saved/GLM_1_AutoML_10_20220803_231402'\n",
    "uploaded_model = h2o.upload_model(model_path)\n",
    "m = h2o.get_model('GLM_1_AutoML_10_20220803_231402')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.varimp_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../leaderboard/SC_regression_Aug3_final_output.csv')\n",
    "df.loc[:,'model_family'] = df.apply(lambda row:add_model_family(row),axis=1)\n",
    "\n",
    "selected_models = df.sort_values(by=f'{evaluation_metrics}_mean',ascending=True).groupby('model_family').first()\n",
    "selected_models = selected_models.sort_values(by=f'{evaluation_metrics}_mean', ascending=True)\n",
    "model_list = selected_models['model_id'].values\n",
    "        \n",
    "# model_list = df['model_id'].values\n",
    "df = h2o.H2OFrame(selected_models)\n",
    "\n",
    "for model in model_list:\n",
    "    model_path = os.path.join('../leaderboard/model_saved/', model)\n",
    "    h2o.upload_model(model_path)\n",
    "    \n",
    "print(df)\n",
    "    \n",
    "h2o.varimp_heatmap(df, test_h2o,num_of_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../leaderboard/SC_binary_classification_Aug4_final_output.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = df\n",
    "df_dev.loc[:,'model_family'] = df_dev.apply(lambda row:add_model_family(row),axis=1)\n",
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addlabels(ax,x,y):\n",
    "    for i in range(len(x)):\n",
    "        ax.text(i, (y[i]+50)//2, y[i], ha = 'center', color='white',fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_bar_plot(df, evaluation_metrics):\n",
    "    selected_models = df.sort_values(by=f'{evaluation_metrics}_mean',ascending=False).groupby('model_family').first()\n",
    "    selected_models = selected_models.sort_values(by=f'{evaluation_metrics}_mean', ascending=False)\n",
    "    y = selected_models[f'{evaluation_metrics}_mean']\n",
    "    x = y.index\n",
    "    y_error = selected_models[f'{evaluation_metrics}_std']\n",
    "    \n",
    "#     x = list(df['model_id'])\n",
    "#     y = df[f'{evaluation_metrics}_mean']\n",
    "    y_1 = [round(a,2) for a in y]\n",
    "#     y_error = df[f'{evaluation_metrics}_std']\n",
    "    y_error_perc = [a*100 for a in y_error]\n",
    "    y_perc = [round(a*100,1) for a in y]\n",
    "    colors = [\"#F9665E\" if i == 0 else \"#799FCB\" for i in range(len(x))]\n",
    "\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "    fig.set_size_inches(20,5)\n",
    "\n",
    "    if evaluation_metrics == 'mse':\n",
    "        ax1.set_ylim(0,max(y+y_error)+0.5)\n",
    "        bars = ax1.bar(x,y_1,color=colors)\n",
    "    elif evaluation_metrics == 'accuracy':\n",
    "        ax1.set_ylim(50,100)\n",
    "        bars = ax1.bar(x,y_perc,color=colors)\n",
    "    else:\n",
    "        bars = ax1.bar(x,y,color=colors)\n",
    "        \n",
    "    addlabels(ax1,x,y_perc)\n",
    "#     ax1.bar_label(bars,label_type='center', color='white', fontsize=12)\n",
    "    ax1.set_xlabel('Model', fontsize=20)\n",
    "    ax1.set_ylabel('Accuracy(%)', fontsize=20)\n",
    "    \n",
    "    ax1.errorbar(x, y_perc, yerr=y_error_perc,\n",
    "              fmt='o', color='orange', ecolor='orange',\n",
    "              elinewidth = 3, capsize=10)\n",
    "    \n",
    "    \n",
    "    ax2.scatter(y,y_error)\n",
    "    for i in range(len(x)):\n",
    "        alignment = 'left' if y[i] < 0.800 else 'right'\n",
    "        ax2.text(x=y[i], y=y_error[i], s=x[i], \n",
    "                 horizontalalignment=alignment, verticalalignment='bottom',\n",
    "                 fontdict=dict(color='black', alpha=0.8, size=16))\n",
    "                    \n",
    "    ax2.plot([min(y), max(y)], [max(y_error), min(y_error)], ls=\"--\", c=\".3\")\n",
    "#     ax2.arrow(min(y), max(y_error), -0.01,0.02)\n",
    "    ax2.set_xlabel(f'{evaluation_metrics}_mean', fontsize=20)\n",
    "    ax2.set_ylabel(f'{evaluation_metrics}_std', fontsize=20)\n",
    "    \n",
    "    plt.setp(ax1.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "    \n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.3, \n",
    "                    hspace=0.4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top_k, top_k_error, test = filter_data(rank_metrix='MSE',challenge='regression',topk=5)\n",
    "error_bar_plot(df_dev, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_bar_plot(df, evaluation_metrics):\n",
    "    selected_models = df.sort_values(by=f'{evaluation_metrics}_mean',ascending=False).groupby('model_family').first()\n",
    "    selected_models = selected_models.sort_values(by=f'{evaluation_metrics}_mean', ascending=False)\n",
    "    y = selected_models[f'{evaluation_metrics}_mean']\n",
    "    x = y.index\n",
    "    y_error = selected_models[f'{evaluation_metrics}_std']\n",
    "    \n",
    "    y_1 = [round(a,2) for a in y]\n",
    "    y_error_perc = [a*100 for a in y_error]\n",
    "    y_perc = [round(a*100,1) for a in y]\n",
    "    colors = [\"#F9665E\" if i == 1 else \"#799FCB\" for i in range(len(x))]\n",
    "\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "    fig.set_size_inches(20,5)\n",
    "\n",
    "    if evaluation_metrics == 'mse':\n",
    "        ax1.set_ylim(0,max(y+y_error)+0.5)\n",
    "        bars = ax1.bar(x,y_1,color=colors)\n",
    "    elif evaluation_metrics == 'auc':\n",
    "        ax1.set_ylim(0,1)\n",
    "        bars = ax1.bar(x,y_1,color=colors)\n",
    "    else:\n",
    "        bars = ax1.bar(x,y,color=colors)\n",
    "        \n",
    "#     addlabels(ax1,x,y_1)\n",
    "    ax1.bar_label(bars,label_type='center', color='white', fontsize=12)\n",
    "    ax1.set_xlabel('Model', fontsize=20)\n",
    "    ax1.set_ylabel('Area under ROC', fontsize=20)\n",
    "    \n",
    "    ax1.errorbar(x, y, yerr=y_error,\n",
    "              fmt='o', color='orange', ecolor='orange',\n",
    "              elinewidth = 3, capsize=10)\n",
    "    \n",
    "    ax2.scatter(y,y_error)\n",
    "    for i in range(len(x)):\n",
    "        alignment = 'left' if y[i] < 0.800 else 'right'\n",
    "        ax2.text(x=y[i], y=y_error[i], s=x[i], \n",
    "                 horizontalalignment=alignment, verticalalignment='bottom',\n",
    "                 fontdict=dict(color='black', alpha=0.8, size=16))\n",
    "                    \n",
    "    ax2.plot([min(y), max(y)], [max(y_error), min(y_error)], ls=\"--\", c=\".3\")\n",
    "#     ax2.arrow(min(y), max(y_error), -0.01,0.02)\n",
    "    ax2.set_xlabel(f'{evaluation_metrics}_mean', fontsize=20)\n",
    "    ax2.set_ylabel(f'{evaluation_metrics}_std', fontsize=20)\n",
    "    \n",
    "    plt.setp(ax1.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "    \n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.3, \n",
    "                    hspace=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k, top_k_error, test = filter_data(rank_metrix='MSE',challenge='regression',topk=5)\n",
    "error_bar_plot(df_dev, 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CoronnaCERTAINDataset(\n",
    "    library_root = 'Dataset/',\n",
    "    challenge = 'binary_classification', #option: regression, regression_delta, classification, binary_classification, regression_delta_binary\n",
    "    dataset = 'CORRONA CERTAIN', \n",
    "    process_approach = 'SC', #option: KVB, SC\n",
    "    imputation = None, #option: SimpleFill, KNN, SoftImpute, BiScaler, NuclearNormMinimization, IterativeImputer, IterativeSVD, None(raw)\n",
    "    patient_group = ['bionaive TNF'], #option: \"all\", \"bioexp nTNF\", \"bionaive TNF\", \"bionaive orencia\", \"KVB\"\n",
    "    drug_group = 'all', #option: \"all\", \"actemra\", \"cimzia\", \"enbrel\", \"humira\", \"orencia\", \"remicade\", \"rituxan\", \"simponi\"\n",
    "    time_points = (0,3), \n",
    "    train_test_rate = 0.8,\n",
    "    remove_low_DAS = True,\n",
    "    save_csv = False, \n",
    "    random_state = 2022,\n",
    "    verbose=False)\n",
    "\n",
    "train, train_loc = dataset.get_train()\n",
    "test, test_loc = dataset.get_test()\n",
    "\n",
    "# Start the H2O cluster (locally)\n",
    "h2o.init()\n",
    "\n",
    "# Import a sample binary outcome train/test set into H2O\n",
    "# train_h2o = h2o.upload_file(str(train_loc))\n",
    "# test_h2o = h2o.upload_file(str(test_loc))\n",
    "train_h2o = h2o.import_file(str(train_loc))\n",
    "test_h2o = h2o.import_file(str(test_loc))\n",
    "\n",
    "# Identify predictors and response\n",
    "x = train_h2o.columns[:-1]\n",
    "# y = \"DAS28_CRP_3M\"\n",
    "y = dataset.target\n",
    "\n",
    "for feature in dataset.categorical:\n",
    "    train_h2o[feature] = train_h2o[feature].asfactor()\n",
    "    test_h2o[feature] = test_h2o[feature].asfactor()\n",
    "if \"classification\" in dataset.challenge:\n",
    "    train_h2o[y] = train_h2o[y].asfactor()\n",
    "    test_h2o[y] = test_h2o[y].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "evaluation_metrics = 'accuracy'\n",
    "model_path_dir = '../leaderboard/model_saved/'\n",
    "model_id_list = list(df_dev.loc[df_dev.groupby('model_family')[f'{evaluation_metrics}_mean'].idxmax()].reset_index(drop=True).sort_values(by=f'{evaluation_metrics}_mean',ascending=False)['model_id'].values)\n",
    "\n",
    "for i, model_id in enumerate(model_id_list):\n",
    "    model_path = os.path.join(model_path_dir,model_id)\n",
    "    uploaded_model = h2o.upload_model(model_path)\n",
    "    m = h2o.get_model(model_id)\n",
    "#     print(m)\n",
    "    regression_pred = m.predict(test_h2o).as_data_frame()['predict']\n",
    "    regression_true = test_h2o.as_data_frame()[dataset.target]\n",
    "    m.model_performance(test_h2o)\n",
    "\n",
    "    X = test.drop(columns=dataset.target)\n",
    "    true = test[dataset.target]\n",
    "    pred = m.predict(test_h2o).as_data_frame()['predict']\n",
    "\n",
    "    baseline = test['DAS28_CRP_0M']\n",
    "\n",
    "    baseline, true, pred = np.array(baseline), np.array(true), np.squeeze(np.array(pred))\n",
    "\n",
    "    print(\"classification accuracy\", Classification_Accuracy(true,pred))\n",
    "    print(\"F1 score\", F1_Score(true,pred))\n",
    "\n",
    "    contingency_matrix = pd.crosstab(true, pred, rownames=['true'], colnames=[\n",
    "                                                 'prediction'], normalize='columns')\n",
    "    plt.figure(i)\n",
    "    sns.heatmap(contingency_matrix.T, annot=True,\n",
    "                fmt='.2f', cmap=\"YlGnBu\", cbar=False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../leaderboard/SC_3_Class_classification_Aug4_final_output.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = df\n",
    "df_dev.loc[:,'model_family'] = df_dev.apply(lambda row:add_model_family(row),axis=1)\n",
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addlabels(ax,x,y):\n",
    "    for i in range(len(x)):\n",
    "        ax.text(i, (y[i])//2, y[i], ha = 'center', color='white',fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_bar_plot(df, evaluation_metrics):\n",
    "    selected_models = df.sort_values(by=f'{evaluation_metrics}_mean',ascending=False).groupby('model_family').first()\n",
    "    selected_models = selected_models.sort_values(by=f'{evaluation_metrics}_mean', ascending=False)\n",
    "    y = selected_models[f'{evaluation_metrics}_mean']\n",
    "    x = y.index\n",
    "    y_error = selected_models[f'{evaluation_metrics}_std']\n",
    "    \n",
    "#     x = list(df['model_id'])\n",
    "#     y = df[f'{evaluation_metrics}_mean']\n",
    "    y_1 = [round(a,2) for a in y]\n",
    "#     y_error = df[f'{evaluation_metrics}_std']\n",
    "    y_error_perc = [a*100 for a in y_error]\n",
    "    y_perc = [round(a*100,1) for a in y]\n",
    "    colors = [\"#F9665E\" if i == 0 else \"#799FCB\" for i in range(len(x))]\n",
    "\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "    fig.set_size_inches(20,5)\n",
    "\n",
    "    if evaluation_metrics == 'mse':\n",
    "        ax1.set_ylim(0,max(y+y_error)+0.5)\n",
    "        bars = ax1.bar(x,y_1,color=colors)\n",
    "    elif evaluation_metrics == 'accuracy':\n",
    "        ax1.set_ylim(0,100)\n",
    "        bars = ax1.bar(x,y_perc,color=colors)\n",
    "    else:\n",
    "        bars = ax1.bar(x,y,color=colors)\n",
    "        \n",
    "    addlabels(ax1,x,y_perc)\n",
    "#     ax1.bar_label(bars,label_type='center', color='white', fontsize=12)\n",
    "    ax1.set_xlabel('Model', fontsize=20)\n",
    "    ax1.set_ylabel('Accuracy(%)', fontsize=20)\n",
    "    \n",
    "    ax1.errorbar(x, y_perc, yerr=y_error_perc,\n",
    "              fmt='o', color='orange', ecolor='orange',\n",
    "              elinewidth = 3, capsize=10)\n",
    "    \n",
    "    \n",
    "    ax2.scatter(y,y_error)\n",
    "    for i in range(len(x)):\n",
    "        alignment = 'left' if y[i] < 0.800 else 'right'\n",
    "        ax2.text(x=y[i], y=y_error[i], s=x[i], \n",
    "                 horizontalalignment=alignment, verticalalignment='bottom',\n",
    "                 fontdict=dict(color='black', alpha=0.8, size=16))\n",
    "                    \n",
    "    ax2.plot([min(y), max(y)], [max(y_error), min(y_error)], ls=\"--\", c=\".3\")\n",
    "#     ax2.arrow(min(y), max(y_error), -0.01,0.02)\n",
    "    ax2.set_xlabel(f'{evaluation_metrics}_mean', fontsize=20)\n",
    "    ax2.set_ylabel(f'{evaluation_metrics}_std', fontsize=20)\n",
    "    \n",
    "    plt.setp(ax1.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "    \n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.3, \n",
    "                    hspace=0.4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k, top_k_error, test = filter_data(rank_metrix='MSE',challenge='regression',topk=5)\n",
    "error_bar_plot(df_dev, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_bar_plot(df, evaluation_metrics):\n",
    "    selected_models = df.sort_values(by=f'{evaluation_metrics}_mean',ascending=False).groupby('model_family').first()\n",
    "    selected_models = selected_models.sort_values(by=f'{evaluation_metrics}_mean', ascending=False)\n",
    "    y = selected_models[f'{evaluation_metrics}_mean']\n",
    "    x = y.index\n",
    "    y_error = selected_models[f'{evaluation_metrics}_std']\n",
    "    \n",
    "    y_1 = [round(a,2) for a in y]\n",
    "    y_error_perc = [a*100 for a in y_error]\n",
    "    y_perc = [round(a*100,1) for a in y]\n",
    "    colors = [\"#F9665E\" if i == 1 else \"#799FCB\" for i in range(len(x))]\n",
    "\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "    fig.set_size_inches(20,5)\n",
    "\n",
    "    if evaluation_metrics == 'mse':\n",
    "        ax1.set_ylim(0,max(y+y_error)+0.5)\n",
    "        bars = ax1.bar(x,y_1,color=colors)\n",
    "    elif evaluation_metrics == 'auc':\n",
    "        ax1.set_ylim(0,1)\n",
    "        bars = ax1.bar(x,y_1,color=colors)\n",
    "    else:\n",
    "        bars = ax1.bar(x,y,color=colors)\n",
    "        \n",
    "#     addlabels(ax1,x,y_1)\n",
    "    ax1.bar_label(bars,label_type='center', color='white', fontsize=12)\n",
    "    ax1.set_xlabel('Model', fontsize=20)\n",
    "    ax1.set_ylabel('Area under ROC', fontsize=20)\n",
    "    \n",
    "    ax1.errorbar(x, y, yerr=y_error,\n",
    "              fmt='o', color='orange', ecolor='orange',\n",
    "              elinewidth = 3, capsize=10)\n",
    "    \n",
    "    ax2.scatter(y,y_error)\n",
    "    for i in range(len(x)):\n",
    "        alignment = 'left' if y[i] < 0.800 else 'right'\n",
    "        ax2.text(x=y[i], y=y_error[i], s=x[i], \n",
    "                 horizontalalignment=alignment, verticalalignment='bottom',\n",
    "                 fontdict=dict(color='black', alpha=0.8, size=16))\n",
    "                    \n",
    "    ax2.plot([min(y), max(y)], [max(y_error), min(y_error)], ls=\"--\", c=\".3\")\n",
    "#     ax2.arrow(min(y), max(y_error), -0.01,0.02)\n",
    "    ax2.set_xlabel(f'{evaluation_metrics}_mean', fontsize=20)\n",
    "    ax2.set_ylabel(f'{evaluation_metrics}_std', fontsize=20)\n",
    "    \n",
    "    plt.setp(ax1.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "    \n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.3, \n",
    "                    hspace=0.2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML_RA_visualization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ADPred_new",
   "language": "python",
   "name": "adpred_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fa226d29700ae65c322d0181ce867828fd74ff498541ec8ff7965e7d33c7878"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
